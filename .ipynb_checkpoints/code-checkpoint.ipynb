{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6f1a12-f424-411a-8c67-187202065b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a1308ff-961d-418b-aba9-bbfe33f02b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(\"runs/detect/train/weights/best.onnx\")# Load the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e67c641c-9077-4e11-9cdb-e5318bfe7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, input_size=640):# Load and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_resized = cv2.resize(image, (input_size, input_size))\n",
    "    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "    image_normalized = image_rgb / 255.0  # Normalize to [0, 1]\n",
    "    image_transposed = np.transpose(image_normalized, (2, 0, 1))  # CHW format\n",
    "    image_input = np.expand_dims(image_transposed, axis=0).astype(np.float32)  # Add batch dimension\n",
    "    return image_input, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c491722-0caf-42c6-8cfb-0166709ee422",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input, original_image = preprocess_image(\"7_jpg.rf.e3a82d5baf4f092e957cdffc65b851ef.jpg\")# Preprocess the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ac8a753-a2d9-424c-b71c-c4a2bdf2cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: image_input})# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f2dff7c-6eb2-4e14-8690-7acd32b6755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process and display results (adjust based on YOLOv11 model specifics)\n",
    "def postprocess(outputs, confidence_threshold=0.5, input_shape=(640, 640), original_shape=None):\n",
    "    boxes, scores, class_ids = [], [], []\n",
    "    for detection in outputs[0][0]:  # Adjust indexing if necessary\n",
    "        score = detection[4]  # Confidence score at index 4\n",
    "        if score > confidence_threshold:\n",
    "            class_id = int(np.argmax(detection[5:]))  # Class ID\n",
    "            x_center, y_center, width, height = detection[:4]  # Bounding box (center x, y, w, h)\n",
    "\n",
    "            # Convert box format from center-width-height to top-left and bottom-right points\n",
    "            x1 = int((x_center - width / 2) * original_shape[1] / input_shape[1])\n",
    "            y1 = int((y_center - height / 2) * original_shape[0] / input_shape[0])\n",
    "            x2 = int((x_center + width / 2) * original_shape[1] / input_shape[1])\n",
    "            y2 = int((y_center + height / 2) * original_shape[0] / input_shape[0])\n",
    "\n",
    "            boxes.append((x1, y1, x2, y2))\n",
    "            scores.append(score)\n",
    "            class_ids.append(class_id)\n",
    "    return boxes, scores, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eecd4fc-2f40-4813-a421-1529e4534a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process the outputs\n",
    "boxes, scores, class_ids = postprocess(outputs, original_shape=original_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84d7c8a3-29f3-4e15-8bf6-299007ddaf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results on the original image\n",
    "for box, score, class_id in zip(boxes, scores, class_ids):\n",
    "    x1, y1, x2, y2 = box\n",
    "    color = (0, 255, 0)  # Green for bounding box\n",
    "    cv2.rectangle(original_image, (x1, y1), (x2, y2), color, 2)\n",
    "    cv2.putText(original_image, f\"Class {class_id} ({score:.2f})\", (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58d82b7c-04e8-47b9-a063-f548d8369ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the image with bounding boxes\n",
    "cv2.imshow(\"Result\", original_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
