{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf723b2e-039f-41fa-b65c-5378d22db2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7f84ae-e22b-4784-b835-826ad002125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.24  Python-3.11.0 torch-2.5.0+cpu CPU (AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\91977\\Desktop\\Piyush\\4th Year\\4-1\\Acads\\BIO-DOP-IMG-YOLO\\datasets\\oral-cancer-3\\valid\\labels.cache... 970 images, 367 backgrounds, 0 corrupt: 100%|██████████| 970/970 [00:00<?, ?it/s]\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 61/61 [04:18<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        970       2021      0.883      0.802      0.876      0.587\n",
      "                cancer        505       1049       0.88      0.773      0.856      0.547\n",
      "             no cancer        512        972      0.886      0.831      0.896      0.628\n",
      "Speed: 3.8ms preprocess, 227.1ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#For 100 epochs-Yolov11 model\n",
    "model = YOLO(\"runs/detect/train_100_epochs/weights/best.pt\")  # Replace with the path to your model file if different\n",
    "metrics = model.val(data=r\"C:\\Users\\91977\\Desktop\\Piyush\\4th Year\\4-1\\Acads\\BIO-DOP-IMG-YOLO\\datasets\\oral-cancer-3\\data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca2f51d-3580-431e-9cba-51421e97977d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Metrics for 100 epochs-Yolov11 model\n",
    "results = metrics.results_dict# Retrieve detection metrics as a dictionary\n",
    "# Mean precision and recall\n",
    "mean_precision = results.get(\"metrics/precision(B)\", None)\n",
    "mean_recall = results.get(\"metrics/recall(B)\", None)\n",
    "mAP50 = results.get(\"metrics/mAP50(B)\", None)\n",
    "mAP75 = metrics.box.map75\n",
    "mAP50_95 = results.get(\"metrics/mAP50-95(B)\", None)\n",
    "fitness = results.get(\"fitness\", None)\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if (precision + recall) == 0:\n",
    "        return 0.0  # Avoid division by zero if precision and recall are both 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "f1_score = calculate_f1_score(0.8828081612498743,0.802026114320279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af2a832-c1b0-414b-9c3d-e214b5b256e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv11 (yolo11n) model was trained over 100 epochs with a batch size of 8, processing images resized to 640x640 pixels. Training utilized a CUDA-enabled RTX 2060 Super GPU\n",
      "Mean Precision: 0.8828081612498743\n",
      "Mean Recall: 0.802026114320279\n",
      "mAP50-95: 0.5872232726921287\n",
      "mAP50: 0.8759128393685293\n",
      "mAP75: 0.6769633293289965\n",
      "Fitness: 0.6160922293597687\n",
      "F1 Score: 0.8404805262142065\n"
     ]
    }
   ],
   "source": [
    "#Printing Metrics for 100 epochs-Yolov11 model\n",
    "print(\"YOLOv11 (yolo11n) model was trained over 100 epochs with a batch size of 8, processing images resized to 640x640 pixels. Training utilized a CUDA-enabled RTX 2060 Super GPU\")\n",
    "print(\"Mean Precision:\", mean_precision) \n",
    "print(\"Mean Recall:\", mean_recall) \n",
    "print(\"mAP50-95:\", mAP50_95)\n",
    "print(\"mAP50:\", mAP50)\n",
    "print(\"mAP75:\", mAP75)\n",
    "print(\"Fitness:\", fitness)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78ef669-cb52-4bd6-9268-b42274942ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.24  Python-3.11.0 torch-2.5.0+cpu CPU (AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\91977\\Desktop\\Piyush\\4th Year\\4-1\\Acads\\BIO-DOP-IM\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        970       2021      0.928      0.844      0.902      0.659\n",
      "                cancer        505       1049      0.928      0.831      0.889      0.619\n",
      "             no cancer        512        972      0.928      0.857      0.915      0.699\n",
      "Speed: 5.8ms preprocess, 275.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#For 150 epochs-Yolov11 model\n",
    "model = YOLO(\"runs/detect/train_150_epochs/weights/best.pt\")  # Replace with the path to your model file if different\n",
    "metrics = model.val(data=r\"C:\\Users\\91977\\Desktop\\Piyush\\4th Year\\4-1\\Acads\\BIO-DOP-IMG-YOLO\\datasets\\oral-cancer-3\\data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee60d2c-3b4e-4f51-9024-e1105a123d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics for 150 epochs-Yolov11 model\n",
    "results = metrics.results_dict# Retrieve detection metrics as a dictionary\n",
    "# Mean precision and recall\n",
    "mean_precision = results.get(\"metrics/precision(B)\", None)\n",
    "mean_recall = results.get(\"metrics/recall(B)\", None)\n",
    "mAP50 = results.get(\"metrics/mAP50(B)\", None)\n",
    "mAP75 = metrics.box.map75\n",
    "mAP50_95 = results.get(\"metrics/mAP50-95(B)\", None)\n",
    "fitness = results.get(\"fitness\", None)\n",
    "f1_score = calculate_f1_score(mean_precision,mean_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "318a027b-bce2-42ce-8ced-a843aeda6738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv11 (yolo11n) model was trained over 150 epochs with a batch size of 8, processing images resized to 640x640 pixels. Training utilized a CUDA-enabled RTX 2060 Super GPU\n",
      "Mean Precision: 0.9276221561902811\n",
      "Mean Recall: 0.8439271967452386\n",
      "mAP50-95: 0.6592978714878767\n",
      "mAP50: 0.9016686318200497\n",
      "mAP75: 0.7627243714606908\n",
      "Fitness: 0.683534947521094\n",
      "F1 Score: 0.8837976369275122\n"
     ]
    }
   ],
   "source": [
    "#Printing Metrics for 150 epochs-Yolov11 model\n",
    "print(\"YOLOv11 (yolo11n) model was trained over 150 epochs with a batch size of 8, processing images resized to 640x640 pixels. Training utilized a CUDA-enabled RTX 2060 Super GPU\")\n",
    "print(\"Mean Precision:\", mean_precision) \n",
    "print(\"Mean Recall:\", mean_recall) \n",
    "print(\"mAP50-95:\", mAP50_95)\n",
    "print(\"mAP50:\", mAP50)\n",
    "print(\"mAP75:\", mAP75)\n",
    "print(\"Fitness:\", fitness)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ff4dbf4-9eda-4a02-8390-0f7c130e5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of model:\n",
    "#A precision of 0.88 suggests that out of all predictions the model made, 88% are correct. This is a high precision, indicating the model makes relatively few false positive predictions.\n",
    "#A recall of 0.80 means that the model is able to correctly detect 80% of the actual objects. While reasonably high, this suggests the model may still be missing some objects (false negatives).\n",
    "#mAP50 (IoU=0.5): Measures average precision when IoU between predicted and ground truth boxes is at least 0.5. A higher value indicates better alignment between predicted and ground truth boxes. It’s a basic metric for object detection performance.\n",
    "    #0.8759: A high mAP50 score indicates that the model is generally good at detecting objects, with the predicted bounding boxes overlapping the ground truth by at least 50%. This suggests the model is fairly accurate and performs well on the basic object detection task.\n",
    "#mAP50-95 (IoU=0.5 to 0.95): Measures average precision across IoU thresholds from 0.5 to 0.95, rewarding models that can predict precise bounding boxes. It's a more rigorous evaluation than mAP50, commonly used in challenges like COCO.\n",
    "    #0.5872: This moderate mAP50-95 value suggests that while the model performs well at the 0.5 IoU threshold, its performance decreases when tested with stricter IoU thresholds. It indicates that the model could improve in predicting more precise bounding boxes, especially for harder-to-detect objects.\n",
    "#mAP75 (IoU=0.75): Measures average precision at a stricter IoU threshold of 0.75. A higher value indicates the model’s ability to make highly accurate predictions, especially for fine details and small objects.\n",
    "    #0.6770: The mAP75 score reflects a more precise model with an IoU threshold of 0.75. While it's not as high as mAP50, it shows that the model can make reasonably accurate predictions, but it struggles slightly with achieving a higher level of precision\n",
    "#The fitness score is a weighted combination of multiple metrics, designed to summarize overall model quality. A score of 0.62 indicates moderate overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c1a61-e331-4900-997f-6f84c599cb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
